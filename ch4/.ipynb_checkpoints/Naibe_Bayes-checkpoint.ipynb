{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n"
     ]
    }
   ],
   "source": [
    "text_data = [['my','dog','has','flea','problems','help','please'],\\\n",
    "           ['maybe','not','take','him','to','dog','park','stupid'],\\\n",
    "           ['my','dalmation','is','so','cute','I','love','him'],\\\n",
    "           ['stop','posting','stupid','worthless','garbage'],\\\n",
    "           ['mr','licks','ate','my','steak','how','to','stop','him'],\\\n",
    "           ['quit','buying','worthless','dog','food','stupid']]\n",
    "print text_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "def create_vocabulary_feature(text_data):\n",
    "    vocabulary_feature = set([])\n",
    "    for row_words in text_data:\n",
    "        vocabulary_feature = vocabulary_feature | set(row_words)\n",
    "    return list(vocabulary_feature)\n",
    "vocabulary_table = create_vocabulary_feature(text_data)\n",
    "print len(vocabulary_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "def words_to_vector(vocabulary_feature, input_words):\n",
    "    words_vector = [0] * len(vocabulary_feature)\n",
    "    for word in input_words:\n",
    "        if word in vocabulary_feature:\n",
    "            words_vector[vocabulary_feature.index(word)] = 1\n",
    "        else:\n",
    "            print \"not in my vocabulary table\"\n",
    "    return words_vector\n",
    "print words_to_vector(vocabulary_table,text_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "from numpy import *\n",
    "def naive_bayers_classfier(train_data, data_class):\n",
    "    data_num = len(train_data)\n",
    "    feature_num = len(train_data[0])\n",
    "    negetive_percent = sum(data_class)/ float(data_num)\n",
    "    positive_word_count = ones(feature_num)\n",
    "    positive_class_word_num = 2.0\n",
    "    negetive_word_count = ones(feature_num)\n",
    "    negetive_class_word_num = 2.0\n",
    "    \n",
    "    for i in range(data_num):\n",
    "        if data_class[i] == 1:\n",
    "            negetive_word_count += train_data[1]\n",
    "            negetive_class_word_num += sum(train_data[1])\n",
    "        else:\n",
    "            positive_word_count += train_data[i]\n",
    "            positive_class_word_num += sum(train_data[i])\n",
    "    negetive_word_freq = log(negetive_word_count / float(negetive_class_word_num))\n",
    "    positive_word_freq = log(positive_word_count / float(positive_class_word_num))\n",
    "    return negetive_percent, negetive_word_freq, positive_word_freq\n",
    "\n",
    "data_class = [0,1,0,1,0,1]\n",
    "train_data = []\n",
    "for row_data in text_data:\n",
    "    train_data.append(words_to_vector(vocabulary_table,row_data))\n",
    "\n",
    "negetive_freq , negetive_word_freq, positive_word_freq = naive_bayers_classfier(train_data, data_class)\n",
    "print len(negetive_word_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is positive label\n"
     ]
    }
   ],
   "source": [
    "def test_naibe_Bay_classifer(test_vector, positive_word_freq,\\\n",
    "                            negetive_word_freq,class_freq):\n",
    "    negetive_freq = sum ([a * b for a in test_vector for b in negetive_word_freq]) + log(class_freq)\n",
    "    positive_freq = sum([a * b for a in test_vector for b in positive_word_freq]) + log(1 - class_freq)\n",
    "\n",
    "    if negetive_freq > positive_freq:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def test():\n",
    "    test_word = ['love','my','dalmation']\n",
    "    test_word_vec = words_to_vector(vocabulary_table, test_word)\n",
    "   \n",
    "    freq = test_naibe_Bay_classifer(test_word_vec,positive_word_freq,negetive_word_freq,negetive_freq)\n",
    "    if freq == 1:\n",
    "        print 'is negetive label'\n",
    "    else:\n",
    "        print 'is positive label'\n",
    "\n",
    "test()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hydrocodone', 'vicodin', 'brand', 'watson', 'vicodin', 'brand', 'watson', 'brand', 'watson', 'noprescription', 'required', 'free', 'express', 'fedex', 'days', 'delivery', 'for', 'over', 'order', 'major', 'credit', 'cards', 'check']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def is_char(ch):\n",
    "    if (ch > 'a' and ch < 'z') or (ch > 'A' and ch < 'Z'):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def parse_text_words(string):\n",
    "    parsed_text = re.split(r'\\W*', string)\n",
    "    return [item.lower() for item in parsed_text if len(item) > 2 \\\n",
    "            and is_char(item[0]) == 1]\n",
    "print parse_text_words(open('email/spam/2.txt').read())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def test_mail_classify():\n",
    "    words_list = []\n",
    "    class_list = []\n",
    "    for i in range(1, 26):\n",
    "        spam_words = parse_text_words(open('email/spam/%d.txt' % i).read())\n",
    "        words_list.append(spam_words)\n",
    "        class_list.append(1)\n",
    "        ham_words = parse_text_words(open('email/ham/%d.txt' %i).read())\n",
    "        words_list.append(ham_words)\n",
    "        class_list.append(0)\n",
    "    train_index = range(0,50)\n",
    "    test_index = []\n",
    "    for i in range(10):\n",
    "        random_index = int(random.uniform(0,len(train_index)))\n",
    "        test_index.append(random_index)\n",
    "        del(train_index[random_index])\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    train_class = []\n",
    "    test_class = []\n",
    "    mail_vocabulary = create_vocabulary_feature(words_list)\n",
    "    \n",
    "    for index in train_index:\n",
    "        train_set.append(words_to_vector(mail_vocabulary, words_list[index]))\n",
    "        train_class.append(class_list[index])\n",
    "    for index in test_index:\n",
    "        test_set.append(words_to_vector(mail_vocabulary, words_list[index]))\n",
    "        test_class.append(class_list[index])\n",
    "    \n",
    "    spam_class_freq, spam_words_freq,no_spam_words_freq = naive_bayers_classfier(train_data, \\\n",
    "                                                                                train_class)\n",
    "    for sample in test_set:\n",
    "    \n",
    "           \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error mail index:  1\n",
      "error mail index:  2\n",
      "error mail index:  4\n",
      "error mail index:  7\n",
      "error rate:   0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Worksoft\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def test_mail_classify():\n",
    "    words_list = []\n",
    "    class_list = []\n",
    "    for i in range(1, 26):\n",
    "        spam_words = parse_text_words(open('email/spam/%d.txt' % i).read())\n",
    "        words_list.append(spam_words)\n",
    "        class_list.append(1)\n",
    "        ham_words = parse_text_words(open('email/ham/%d.txt' %i).read())\n",
    "        words_list.append(ham_words)\n",
    "        class_list.append(0)\n",
    "    train_index = range(0,50)\n",
    "    test_index = []\n",
    "    for i in range(10):\n",
    "        random_index = int(random.uniform(0,len(train_index)))\n",
    "        test_index.append(random_index)\n",
    "        del(train_index[random_index])\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    train_class = []\n",
    "    test_class = []\n",
    "    mail_vocabulary = create_vocabulary_feature(words_list)\n",
    "    \n",
    "    for index in train_index:\n",
    "        train_set.append(words_to_vector(mail_vocabulary, words_list[index]))\n",
    "        train_class.append(class_list[index])\n",
    "    for index in test_index:\n",
    "        test_set.append(words_to_vector(mail_vocabulary, words_list[index]))\n",
    "        test_class.append(class_list[index])\n",
    "    \n",
    "    spam_class_freq, spam_words_freq,no_spam_words_freq = naive_bayers_classfier(train_data, \\\n",
    "                                                                                 train_class)\n",
    "    error = 0.00\n",
    "    for sample in test_set:\n",
    "        res = test_naibe_Bay_classifer(sample, no_spam_words_freq,spam_words_freq, spam_class_freq)\n",
    "        if res != test_class[test_set.index(sample)]:\n",
    "            error += 1.0\n",
    "            print 'error mail index: ', test_set.index(sample)\n",
    "    print 'error rate:  ',float( error / len(test_set))\n",
    "\n",
    "test_mail_classify()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import feedparser\n",
    "ny = feedparser.parse('http://newyork.craigslist.org/stp/index.rss')\n",
    "ny['entries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
